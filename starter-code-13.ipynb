{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>urlid</th>\n",
       "      <th>boilerplate</th>\n",
       "      <th>alchemy_category</th>\n",
       "      <th>alchemy_category_score</th>\n",
       "      <th>avglinksize</th>\n",
       "      <th>commonlinkratio_1</th>\n",
       "      <th>commonlinkratio_2</th>\n",
       "      <th>commonlinkratio_3</th>\n",
       "      <th>commonlinkratio_4</th>\n",
       "      <th>...</th>\n",
       "      <th>linkwordscore</th>\n",
       "      <th>news_front_page</th>\n",
       "      <th>non_markup_alphanum_characters</th>\n",
       "      <th>numberOfLinks</th>\n",
       "      <th>numwords_in_url</th>\n",
       "      <th>parametrizedLinkRatio</th>\n",
       "      <th>spelling_errors_ratio</th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.bloomberg.com/news/2010-12-23/ibm-p...</td>\n",
       "      <td>4042</td>\n",
       "      <td>{\"title\":\"IBM Sees Holographic Calls Air Breat...</td>\n",
       "      <td>business</td>\n",
       "      <td>0.789131</td>\n",
       "      <td>2.055556</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.205882</td>\n",
       "      <td>0.047059</td>\n",
       "      <td>0.023529</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>5424</td>\n",
       "      <td>170</td>\n",
       "      <td>8</td>\n",
       "      <td>0.152941</td>\n",
       "      <td>0.07913</td>\n",
       "      <td>0</td>\n",
       "      <td>IBM Sees Holographic Calls Air Breathing Batte...</td>\n",
       "      <td>A sign stands outside the International Busine...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  urlid  \\\n",
       "0  http://www.bloomberg.com/news/2010-12-23/ibm-p...   4042   \n",
       "\n",
       "                                         boilerplate alchemy_category  \\\n",
       "0  {\"title\":\"IBM Sees Holographic Calls Air Breat...         business   \n",
       "\n",
       "  alchemy_category_score  avglinksize  commonlinkratio_1  commonlinkratio_2  \\\n",
       "0               0.789131     2.055556           0.676471           0.205882   \n",
       "\n",
       "   commonlinkratio_3  commonlinkratio_4  \\\n",
       "0           0.047059           0.023529   \n",
       "\n",
       "                         ...                          linkwordscore  \\\n",
       "0                        ...                                     24   \n",
       "\n",
       "   news_front_page  non_markup_alphanum_characters  numberOfLinks  \\\n",
       "0                0                            5424            170   \n",
       "\n",
       "   numwords_in_url  parametrizedLinkRatio  spelling_errors_ratio label  \\\n",
       "0                8               0.152941                0.07913     0   \n",
       "\n",
       "                                               title  \\\n",
       "0  IBM Sees Holographic Calls Air Breathing Batte...   \n",
       "\n",
       "                                                body  \n",
       "0  A sign stands outside the International Busine...  \n",
       "\n",
       "[1 rows x 29 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "data = pd.read_csv(\"stumbleupon.tsv\", sep='\\t')\n",
    "data['title'] = data.boilerplate.map(lambda x: json.loads(x).get('title', ''))\n",
    "data['body'] = data.boilerplate.map(lambda x: json.loads(x).get('body', ''))\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting \"Greenness\" Of Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset comes from [stumbleupon](https://www.stumbleupon.com/), a web page recommender.  \n",
    "\n",
    "A description of the columns is below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FieldName|Type|Description\n",
    "---------|----|-----------\n",
    "url|string|Url of the webpage to be classified\n",
    "title|string|Title of the article\n",
    "body|string|Body text of article\n",
    "urlid|integer| StumbleUpon's unique identifier for each url\n",
    "boilerplate|json|Boilerplate text\n",
    "alchemy_category|string|Alchemy category (per the publicly available Alchemy API found at www.alchemyapi.com)\n",
    "alchemy_category_score|double|Alchemy category score (per the publicly available Alchemy API found at www.alchemyapi.com)\n",
    "avglinksize| double|Average number of words in each link\n",
    "commonlinkratio_1|double|# of links sharing at least 1 word with 1 other links / # of links\n",
    "commonlinkratio_2|double|# of links sharing at least 1 word with 2 other links / # of links\n",
    "commonlinkratio_3|double|# of links sharing at least 1 word with 3 other links / # of links\n",
    "commonlinkratio_4|double|# of links sharing at least 1 word with 4 other links / # of links\n",
    "compression_ratio|double|Compression achieved on this page via gzip (measure of redundancy)\n",
    "embed_ratio|double|Count of number of <embed> usage\n",
    "frameBased|integer (0 or 1)|A page is frame-based (1) if it has no body markup but have a frameset markup\n",
    "frameTagRatio|double|Ratio of iframe markups over total number of markups\n",
    "hasDomainLink|integer (0 or 1)|True (1) if it contains an <a> with an url with domain\n",
    "html_ratio|double|Ratio of tags vs text in the page\n",
    "image_ratio|double|Ratio of <img> tags vs text in the page\n",
    "is_news|integer (0 or 1) | True (1) if StumbleUpon's news classifier determines that this webpage is news\n",
    "lengthyLinkDomain| integer (0 or 1)|True (1) if at least 3 <a> 's text contains more than 30 alphanumeric characters\n",
    "linkwordscore|double|Percentage of words on the page that are in hyperlink's text\n",
    "news_front_page| integer (0 or 1)|True (1) if StumbleUpon's news classifier determines that this webpage is front-page news\n",
    "non_markup_alphanum_characters|integer| Page's text's number of alphanumeric characters\n",
    "numberOfLinks|integer Number of <a>|markups\n",
    "numwords_in_url| double|Number of words in url\n",
    "parametrizedLinkRatio|double|A link is parametrized if it's url contains parameters or has an attached onClick event\n",
    "spelling_errors_ratio|double|Ratio of words not found in wiki (considered to be a spelling mistake)\n",
    "label|integer (0 or 1)|User-determined label. Either evergreen (1) or non-evergreen (0); available for train.tsv only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Let's try extracting some of the text content.\n",
    "> ### Create a feature for the title containing 'recipe'. Is the % of evegreen websites higher or lower on pages that have recipe in the the title?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Option 1: Create a function to check for this\n",
    "\n",
    "# def has_recipe(text_in):\n",
    "#     try:\n",
    "#         if 'recipe' in str(text_in).lower():\n",
    "#             return 1\n",
    "#         else:\n",
    "#             return 0\n",
    "#     except: \n",
    "#         return 0\n",
    "        \n",
    "# data['recipe'] = data['title'].map(has_recipe)\n",
    "\n",
    "# Option 2: lambda functions\n",
    "\n",
    "#data['recipe'] = data['title'].map(lambda t: 1 if 'recipe' in str(t).lower() else 0)\n",
    "\n",
    "\n",
    "# Option 3: string functions\n",
    "data['recipe'] = data['title'].str.contains('recipe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BeckyC\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV AUC [ 0.58417488  0.58045567  0.60179258  0.58135322  0.58409004], Average AUC 0.5863732768041588\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "\n",
    "model = DecisionTreeClassifier(max_depth =3) ## Max depth is optional and is used to generalise the model \n",
    "                                                ## not adding a depth means that the tree will keep going until its explained \n",
    "                                                ## every observation as best it can.\n",
    "\n",
    "X = data[['image_ratio', 'html_ratio', 'recipe', 'label']].dropna()\n",
    "y = X['label']\n",
    "X.drop('label', axis=1, inplace=True) ## removed label from the data set as this is the outcome.\n",
    "    \n",
    "    \n",
    "# Fits the model\n",
    "model.fit(X, y)\n",
    "\n",
    "scores = cross_val_score(model, X, y, scoring='roc_auc', cv=5) # output auc based on 5 cross validation folds.\n",
    "print('CV AUC {}, Average AUC {}'.format(scores, scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV AUC [ 0.60727276  0.61441426  0.64510128  0.60396715  0.61921124], Average AUC 0.6179933383259505\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_estimators = 100, oob_score= True, max_depth=3) ## using random forest, create 20 decision trees\n",
    "                                                                    ## oob = the mean score of the third of obervations which \n",
    "                                                                    ## were unseen. I.e. return auc for the test set not the learn set.\n",
    "    \n",
    "model.fit(X, y)\n",
    "\n",
    "scores = cross_val_score(model, X, y, scoring='roc_auc', cv=5)\n",
    "print('CV AUC {}, Average AUC {}'.format(scores, scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:  \n",
    "random forest consistently produces a higher AUC than one decision tree, 0.57 vs 0.54.  When a max_depth of 3 was added the accuracy improved further (tree = 0.58, forest = 0.61) \n",
    "NB AUC is the score for the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Demo: Use of Spacy\n",
    " \n",
    " NB I wasn't able to install spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-9865d8b3cda0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;31m## python -m spacy.en.download --force all\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0men\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEnglish\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mnlp_toolkit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEnglish\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "## I need to install c++ and then run this from the cmd line;\n",
    "## https://www.microsoft.com/en-us/download/confirmation.aspx?id=44266\n",
    "## pip install spacy\n",
    "## python -m spacy.en.download --force all\n",
    "    \n",
    "from spacy.en import English\n",
    "nlp_toolkit = English()\n",
    "\n",
    "title = \"IBM sees holographic calls, air breathing batteries\"\n",
    "parsed = nlp_toolkit(title)\n",
    "\n",
    "for (i, word) in enumerate(parsed):\n",
    "    print(\"Word: {}\".format(word))\n",
    "    print(\"\\t Phrase type: {}\".format(word.dep_))\n",
    "    print(\"\\t Is the word a known entity type? {}\".format(word.ent_type_  if word.ent_type_ else \"No\"))\n",
    "    print(\"\\t Lemma: {}\".format(word.lemma_))\n",
    "    print(\"\\t Parent of this word: {}\".format(word.head.lemma_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp_toolkit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-3b40b4658a71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Tom likes eating food\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mparsed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp_toolkit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Word: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nlp_toolkit' is not defined"
     ]
    }
   ],
   "source": [
    "title = \"Tom likes eating food\"\n",
    "parsed = nlp_toolkit(title)\n",
    "\n",
    "for (i, word) in enumerate(parsed):\n",
    "    print(\"Word: {}\".format(word))\n",
    "    print(\"\\t Phrase type: {}\".format(word.dep_))\n",
    "    print(\"\\t Is the word a known entity type? {}\".format(word.ent_type_  if word.ent_type_ else \"No\"))\n",
    "    print(\"\\t Lemma: {}\".format(word.lemma_))\n",
    "    print(\"\\t Parent of this word: {}\".format(word.head.lemma_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Demo: Use of the Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titles = data['title'].fillna('') # fill nulls with an emptry string.\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features = 1000, # max number of words to convert into features - in alphabetical order.\n",
    "                             ngram_range=(1, 2), # a gram is a word, 1gram is an individual word, \"I\", \"like\"\n",
    "                                                 # 2gram means go 2 by 2 in your sentence. \"I like\"\n",
    "                             stop_words='english', # get rid of words like; and, or, it etc\n",
    "                             binary=True) # uses 1 or 0 if true, uses frequency if false.\n",
    "\n",
    "# Use `fit` to learn the vocabulary of the titles\n",
    "vectorizer.fit(titles)\n",
    "\n",
    "# Use `tranform` to generate the sample X word matrix - one column per feature (word or n-grams)\n",
    "X = vectorizer.transform(titles) # creates an array of all the titles and all of the features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Tf Idf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features = 1000, # max number of words to convert into features - in alphabetical order.\n",
    "                             ngram_range=(1, 1), # a gram is a word, 1gram is an individual word, \"I\", \"like\"\n",
    "                                                 # 2gram means go 2 by 2 in your sentence. \"I like\"\n",
    "                             stop_words='english', # get rid of words like; and, or, it etc\n",
    "                             binary=True) # uses 1 or 0 if true, uses frequency if false.\n",
    "\n",
    "# Use `fit` to learn the vocabulary of the titles\n",
    "vectorizer.fit(titles)\n",
    "\n",
    "# Use `tranform` to generate the sample X word matrix - one column per feature (word or n-grams)\n",
    "X = vectorizer.transform(titles) # creates an array of all the titles and all of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    IBM Sees Holographic Calls Air Breathing Batte...\n",
      "1    The Fully Electronic Futuristic Starting Gun T...\n",
      "2    Fruits that Fight the Flu fruits that fight th...\n",
      "3                  10 Foolproof Tips for Better Sleep \n",
      "4    The 50 Coolest Jerseys You Didn t Know Existed...\n",
      "Name: title, dtype: object\n",
      "  (0, 43)\t1\n",
      "  (2, 209)\t1\n",
      "  (2, 357)\t1\n",
      "  (2, 384)\t1\n",
      "  (2, 435)\t1\n",
      "  (2, 564)\t1\n",
      "  (2, 565)\t1\n",
      "  (3, 1)\t1\n",
      "  (3, 102)\t1\n",
      "  (3, 797)\t1\n",
      "  (3, 907)\t1\n",
      "  (4, 34)\t1\n",
      "  (4, 240)\t1\n",
      "  (4, 504)\t1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'air'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(titles.head())\n",
    "print(X[0:5]) # sample of X \n",
    "## the vectorizer creates a set of features from the text \n",
    "vectorizer.get_feature_names()[43] # this returns the 43rd feature = air.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Demo: Build a random forest model to predict evergreeness of a website using the title features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV AUC [ 0.78907813  0.7983027   0.80518284], Average AUC 0.7975212258535441\n"
     ]
    }
   ],
   "source": [
    "## Now build a random forest using the new text features we've just created using the count vectorizer\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_estimators = 20) # ensemble 20 decision trees into a forest.\n",
    "#, oob_score= True, max_depth=3 \n",
    "\n",
    "# Use `fit` to learn the vocabulary of the titles\n",
    "vectorizer.fit(titles)\n",
    "\n",
    "# Use `tranform` to generate the sample X word matrix - one column per feature (word or n-grams)\n",
    "X = vectorizer.transform(titles).toarray()\n",
    "y = data['label']\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "scores = cross_val_score(model, X, y, scoring='roc_auc') ## cv not used, but notice that the model has K3 by default.\n",
    "print('CV AUC {}, Average AUC {}'.format(scores, scores.mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Build a random forest model to predict evergreeness of a website using the title features and quantitative features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>10</th>\n",
       "      <th>10 best</th>\n",
       "      <th>10 things</th>\n",
       "      <th>10 ways</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>101 cookbooks</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>...</th>\n",
       "      <th>year</th>\n",
       "      <th>year old</th>\n",
       "      <th>years</th>\n",
       "      <th>yes</th>\n",
       "      <th>york</th>\n",
       "      <th>york best</th>\n",
       "      <th>york village</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yummy</th>\n",
       "      <th>zucchini</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   000  10  10 best  10 things  10 ways  100  101  101 cookbooks  11  12  \\\n",
       "0    0   0        0          0        0    0    0              0   0   0   \n",
       "1    0   0        0          0        0    0    0              0   0   0   \n",
       "2    0   0        0          0        0    0    0              0   0   0   \n",
       "3    0   1        0          0        0    0    0              0   0   0   \n",
       "4    0   0        0          0        0    0    0              0   0   0   \n",
       "\n",
       "     ...     year  year old  years  yes  york  york best  york village  \\\n",
       "0    ...        0         0      0    0     0          0             0   \n",
       "1    ...        0         0      0    0     0          0             0   \n",
       "2    ...        0         0      0    0     0          0             0   \n",
       "3    ...        0         0      0    0     0          0             0   \n",
       "4    ...        0         0      0    0     0          0             0   \n",
       "\n",
       "   youtube  yummy  zucchini  \n",
       "0        0      0         0  \n",
       "1        0      0         0  \n",
       "2        0      0         0  \n",
       "3        0      0         0  \n",
       "4        0      0         0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a data frame from the array X\n",
    "dfX = pd.DataFrame(columns = vectorizer.get_feature_names(), data = X.toarray())\n",
    "dfX.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>10</th>\n",
       "      <th>10 best</th>\n",
       "      <th>10 things</th>\n",
       "      <th>10 ways</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>101 cookbooks</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>...</th>\n",
       "      <th>years</th>\n",
       "      <th>yes</th>\n",
       "      <th>york</th>\n",
       "      <th>york best</th>\n",
       "      <th>york village</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yummy</th>\n",
       "      <th>zucchini</th>\n",
       "      <th>commonlinkratio_1</th>\n",
       "      <th>image_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.003883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.508021</td>\n",
       "      <td>0.088652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.562016</td>\n",
       "      <td>0.120536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.035343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.050473</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1002 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   000  10  10 best  10 things  10 ways  100  101  101 cookbooks  11  12  \\\n",
       "0    0   0        0          0        0    0    0              0   0   0   \n",
       "1    0   0        0          0        0    0    0              0   0   0   \n",
       "2    0   0        0          0        0    0    0              0   0   0   \n",
       "3    0   1        0          0        0    0    0              0   0   0   \n",
       "4    0   0        0          0        0    0    0              0   0   0   \n",
       "\n",
       "      ...       years  yes  york  york best  york village  youtube  yummy  \\\n",
       "0     ...           0    0     0          0             0        0      0   \n",
       "1     ...           0    0     0          0             0        0      0   \n",
       "2     ...           0    0     0          0             0        0      0   \n",
       "3     ...           0    0     0          0             0        0      0   \n",
       "4     ...           0    0     0          0             0        0      0   \n",
       "\n",
       "   zucchini  commonlinkratio_1  image_ratio  \n",
       "0         0           0.676471     0.003883  \n",
       "1         0           0.508021     0.088652  \n",
       "2         0           0.562016     0.120536  \n",
       "3         0           0.400000     0.035343  \n",
       "4         0           0.500000     0.050473  \n",
       "\n",
       "[5 rows x 1002 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## adding other continuous features from the original dataset \n",
    "dfX['commonlinkratio_1'] = data['commonlinkratio_1']\n",
    "dfX['image_ratio'] = data['image_ratio']\n",
    "dfX.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=100, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "model = RandomForestClassifier(n_estimators = 100) # oob_score= True, max_depth=3\n",
    "y= data['label'] # label is the outcome, 0 = not evergreen, 1 = evergreen\n",
    "\n",
    "model.fit(dfX,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV AUC[ 0.79672722  0.81099242  0.80487462], Average AUC 0.8041980882123037\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>Importance Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>commonlinkratio_1</td>\n",
       "      <td>0.139046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>image_ratio</td>\n",
       "      <td>0.098861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>recipe</td>\n",
       "      <td>0.040609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>recipes</td>\n",
       "      <td>0.019259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>chocolate</td>\n",
       "      <td>0.013780</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Features  Importance Score\n",
       "1000  commonlinkratio_1          0.139046\n",
       "1001        image_ratio          0.098861\n",
       "715              recipe          0.040609\n",
       "721             recipes          0.019259\n",
       "192           chocolate          0.013780"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(model,dfX,y, scoring = 'roc_auc') ## if cv=none, then 3 folds by default.\n",
    "print('CV AUC{}, Average AUC {}'.format(scores, scores.mean()))\n",
    "\n",
    "\n",
    "## Feature importance \n",
    "all_feature_names = vectorizer.get_feature_names() +  ['commonlinkratio_1','image_ratio']\n",
    "feature_importances = pd.DataFrame({'Features':all_feature_names,'Importance Score' : model.feature_importances_})\n",
    "feature_importances.sort_values('Importance Score', ascending = False).head() ## PRINT THE TOP 5 FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation: the new quant features are better than the text, but the AUC only crept up from 0.79 to 0.8.  \n",
    "Try using different max_depth and change the vectoriser from count to Tf Idf.. can i write a For loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Exercise: Build a random forest model to predict evergreeness of a website using the body features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "body_text = data['body'].fillna('')\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features = 1000, # max number of words to convert into features - in alphabetical order.\n",
    "                             ngram_range=(1, 1), # a gram is a word, 1gram is an individual word, \"I\", \"like\"\n",
    "                                                 # 2gram means go 2 by 2 in your sentence. \"I like\"\n",
    "                             stop_words='english', # get rid of words like; and, or, it etc\n",
    "                             binary=False) # uses 1 or 0 if true, uses frequency if false.\n",
    "\n",
    "vectorizer.fit(body_text) # creates an array of all the titles and all of the features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    A sign stands outside the International Busine...\n",
       "1    And that can be carried on a plane without the...\n",
       "2    Apples The most popular source of antioxidants...\n",
       "3    There was a period in my life when I had a lot...\n",
       "4    Jersey sales is a curious business Whether you...\n",
       "Name: body, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['body'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# store new features in an array \n",
    "X = vectorizer.transform(body_text).toarray()\n",
    "\n",
    "model.fit(X,y)\n",
    "# now run the random forests model (model set above)\n",
    "scores = cross_val_score(model,X,y, scoring = 'roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV AUC[ 0.84935196  0.86143478  0.85316585], Average AUC 0.8546508635137885\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>Importance Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>recipe</td>\n",
       "      <td>0.034220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>recipes</td>\n",
       "      <td>0.028128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>add</td>\n",
       "      <td>0.015041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>food</td>\n",
       "      <td>0.013831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>salt</td>\n",
       "      <td>0.013284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854</th>\n",
       "      <td>sugar</td>\n",
       "      <td>0.013236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>cooking</td>\n",
       "      <td>0.012925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>bowl</td>\n",
       "      <td>0.011749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>baking</td>\n",
       "      <td>0.010653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>butter</td>\n",
       "      <td>0.010354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Features  Importance Score\n",
       "716   recipe          0.034220\n",
       "717  recipes          0.028128\n",
       "46       add          0.015041\n",
       "351     food          0.013831\n",
       "751     salt          0.013284\n",
       "854    sugar          0.013236\n",
       "217  cooking          0.012925\n",
       "122     bowl          0.011749\n",
       "87    baking          0.010653\n",
       "134   butter          0.010354"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output the auc for evaluation\n",
    "print('CV AUC{}, Average AUC {}'.format(scores, scores.mean()))\n",
    "\n",
    "# print the top 10 features:\n",
    "all_feature_names = vectorizer.get_feature_names() \n",
    "feature_importances = pd.DataFrame({'Features':all_feature_names,'Importance Score' : model.feature_importances_})\n",
    "feature_importances.sort_values('Importance Score', ascending = False).head(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorizing (using term freq minus document freq) for the first 1000 words from the \"body\" field in the original evergreen dataset results in an improved AUC, increasing from 0.8 to 0.85. Recipe and Recipes are the most important features for determining evergreenness.  \n",
    "I tested using td-idf binary=True or False, there wasn't a notable change in the AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Exercise: Use `TfIdfVectorizer` instead of `CountVectorizer` - is this an improvement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1000, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) CV AUC[ 0.84444675  0.85923188  0.85184558], Average AUC 0.8518414028449565\n",
      "TfidfVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1000, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) CV AUC[ 0.84617694  0.86370389  0.85113517], Average AUC 0.8536719968524481\n"
     ]
    }
   ],
   "source": [
    "\n",
    "countV = CountVectorizer(max_features = 1000, # max number of words to convert into features - in alphabetical order.\n",
    "                             ngram_range=(1, 1), \n",
    "                             stop_words='english', \n",
    "                             binary=True) # uses 1 or 0 if true, uses frequency if false.\n",
    "\n",
    "tfV = TfidfVectorizer(max_features = 1000, # max number of words to convert into features - in alphabetical order.\n",
    "                             ngram_range=(1, 1), # a gram is a word, 1gram is an individual word, \"I\", \"like\"\n",
    "                                                 # 2gram means go 2 by 2 in your sentence. \"I like\"\n",
    "                             stop_words='english', # get rid of words like; and, or, it etc\n",
    "                             binary=True) # uses 1 or 0 if true, uses frequency if false.\n",
    "\n",
    "vectorizers =  [countV,tfV]\n",
    "\n",
    "for v in vectorizers:\n",
    "    v.fit(body_text) \n",
    "    X = v.transform(body_text).toarray()\n",
    "    model.fit(X,y)\n",
    "    scores = cross_val_score(model,X,y, scoring = 'roc_auc')\n",
    "    print(v, 'CV AUC{}, Average AUC {}'.format(scores, scores.mean()))\n",
    "#     return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above confirms that the AUC is marginally higher when using the TFiDF vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x16e43710>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAESCAYAAADjS5I+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGRSSpKL/D8b5OcCwXf2sAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xd23c320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "\n",
    "#Plotting the ROC for the winning model... \n",
    "probas = model.predict_proba(X)\n",
    "\n",
    "\n",
    "plt.plot(roc_curve(y, probas[:,1])[0], # the roc curve function returns tpr and fpr, the first column [0] is fpr\n",
    "         roc_curve(y, probas[:,1])[1],) # [1] is the tpr\n",
    "plt.xlabel('FPR', fontsize=18)\n",
    "plt.ylabel('TPR', fontsize=16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
